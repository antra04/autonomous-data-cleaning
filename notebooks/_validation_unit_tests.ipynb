{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a23e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned dataset shape: (891, 15)\n",
      "üéØ Completeness: 100.00%\n",
      "üéØ Duplicate rows: 0\n",
      "üéØ Outlier summary: {'PassengerId': {'below_1pct': np.int64(9), 'above_99pct': np.int64(9)}, 'Survived': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'Pclass': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'Age': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'SibSp': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'Parch': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'Fare': {'below_1pct': np.int64(0), 'above_99pct': np.int64(9)}, 'outlier_zscore': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'outlier_iforest': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}, 'outlier_lstm': {'below_1pct': np.int64(0), 'above_99pct': np.int64(0)}}\n",
      "üß† Loading lightweight CPU model: distilgpt2 (fast & small)...\n",
      "WARNING:tensorflow:From c:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Report saved to:\n",
      " - C:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\reports\\ai_data_quality_report_20251007_193351.txt\n",
      " - C:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\reports\\ai_data_quality_report_20251007_193351.md\n",
      " - C:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\reports\\ai_data_quality_report_20251007_193351.html\n",
      "\n",
      "üéâ All files written successfully in UTF-8 encoding!\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# üß† Autonomous Data Quality Report Generator (Free AI)\n",
    "# ======================================================\n",
    "# Model: distilgpt2 (Lightweight CPU-friendly)\n",
    "# Author: Antra Tiwari\n",
    "# ======================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load cleaned dataset\n",
    "# -------------------------------\n",
    "clean_path = r\"C:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\data\\processed\\train_clean.csv\"\n",
    "\n",
    "if not os.path.exists(clean_path):\n",
    "    raise FileNotFoundError(f\"‚ùå File not found at: {clean_path}\")\n",
    "\n",
    "clean_df = pd.read_csv(clean_path)\n",
    "print(f\"‚úÖ Cleaned dataset shape: {clean_df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Data quality metrics\n",
    "# -------------------------------\n",
    "completeness = clean_df.notnull().mean().mean() * 100\n",
    "duplicates = clean_df.duplicated().sum()\n",
    "\n",
    "# Detect outliers\n",
    "numeric_cols = clean_df.select_dtypes(include=np.number).columns\n",
    "outlier_summary = {}\n",
    "for col in numeric_cols:\n",
    "    q_low = clean_df[col].quantile(0.01)\n",
    "    q_high = clean_df[col].quantile(0.99)\n",
    "    below = (clean_df[col] < q_low).sum()\n",
    "    above = (clean_df[col] > q_high).sum()\n",
    "    outlier_summary[col] = {\"below_1pct\": below, \"above_99pct\": above}\n",
    "\n",
    "print(f\"üéØ Completeness: {completeness:.2f}%\")\n",
    "print(f\"üéØ Duplicate rows: {duplicates}\")\n",
    "print(f\"üéØ Outlier summary: {outlier_summary}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load AI model (local)\n",
    "# -------------------------------\n",
    "print(\"üß† Loading lightweight CPU model: distilgpt2 (fast & small)...\")\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Generate AI report\n",
    "# -------------------------------\n",
    "prompt = f\"\"\"\n",
    "You are a professional data quality analyst. \n",
    "Based on the dataset metrics below, generate a clear and structured 3-paragraph report \n",
    "with insights and actionable recommendations.\n",
    "\n",
    "Dataset shape: {clean_df.shape}\n",
    "Completeness: {completeness:.2f}%\n",
    "Duplicate rows: {duplicates}\n",
    "Numeric Columns Summary: {outlier_summary}\n",
    "\"\"\"\n",
    "\n",
    "ai_output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=350,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=50256\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Save report files\n",
    "# -------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_dir = r\"C:\\Users\\Antra Tiwari\\OneDrive\\Desktop\\Autonomous data cleaning\\reports\"\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "report_path_txt = os.path.join(report_dir, f\"ai_data_quality_report_{timestamp}.txt\")\n",
    "report_path_md = os.path.join(report_dir, f\"ai_data_quality_report_{timestamp}.md\")\n",
    "report_path_html = os.path.join(report_dir, f\"ai_data_quality_report_{timestamp}.html\")\n",
    "\n",
    "report_content = f\"\"\"\n",
    "# üßæ AI-Generated Data Quality Report\n",
    "\n",
    "**Generated on:** {timestamp}  \n",
    "**Dataset Shape:** {clean_df.shape}  \n",
    "**Completeness:** {completeness:.2f}%  \n",
    "**Duplicate Rows:** {duplicates}  \n",
    "\n",
    "## üìä Outlier Summary\n",
    "{outlier_summary}\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ AI Insights\n",
    "{ai_output}\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **End of Report**\n",
    "\"\"\"\n",
    "\n",
    "# Save all versions safely (UTF-8 encoding fix)\n",
    "for path in [report_path_txt, report_path_md, report_path_html]:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_content)\n",
    "\n",
    "print(f\"üíæ Report saved to:\\n - {report_path_txt}\\n - {report_path_md}\\n - {report_path_html}\")\n",
    "print(\"\\nüéâ All files written successfully in UTF-8 encoding!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

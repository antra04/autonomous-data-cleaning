{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59945d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n",
      "Train shape: (891, 12)\n",
      "Anomaly shape: (891, 4)\n",
      "Merged shape: (891, 15)\n",
      "Columns after merge: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'outlier_zscore', 'outlier_iforest', 'outlier_lstm']\n",
      "âœ… Missing values imputed!\n",
      "âš  Found 0 duplicate rows\n",
      "âœ… Duplicates removed. New shape: (891, 15)\n",
      "âš  Found 65 outliers flagged by outlier_zscore\n",
      "âš  Found 45 outliers flagged by outlier_iforest\n",
      "âš  Found 45 outliers flagged by outlier_lstm\n",
      "âœ… Outliers capped at 1stâ€“99th percentiles!\n",
      "ğŸ“ Data quality report saved to ../data/processed/data_quality_report.txt\n",
      "ğŸ¯ Final cleaned dataset saved to ../data/processed/train_clean.csv\n",
      "Final shape: (891, 15)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Day 3: Imputation & Cleaning + Outlier & Duplicate Handling\n",
    "# -------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "print(\"âœ… Libraries loaded\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load datasets\n",
    "# -------------------------------\n",
    "train_path = \"../data/raw/train.csv\"\n",
    "anomaly_path = \"../data/metadata/anomaly_flags.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "anomaly_flags = pd.read_csv(anomaly_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Anomaly shape:\", anomaly_flags.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Merge anomaly flags\n",
    "# -------------------------------\n",
    "if \"PassengerId\" in train_df.columns and \"PassengerId\" in anomaly_flags.columns:\n",
    "    merged_df = train_df.merge(anomaly_flags, on=\"PassengerId\", how=\"left\")\n",
    "else:\n",
    "    raise KeyError(\"âŒ PassengerId column missing in either train.csv or anomaly_flags.csv\")\n",
    "\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "print(\"Columns after merge:\", merged_df.columns.tolist())\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Handle missing values\n",
    "# -------------------------------\n",
    "num_cols = merged_df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = merged_df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Mean imputation for numeric\n",
    "num_imputer = SimpleImputer(strategy=\"mean\")\n",
    "merged_df[num_cols] = num_imputer.fit_transform(merged_df[num_cols])\n",
    "\n",
    "# Mode imputation for categorical\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "merged_df[cat_cols] = cat_imputer.fit_transform(merged_df[cat_cols])\n",
    "\n",
    "print(\"âœ… Missing values imputed!\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Duplicate removal\n",
    "# -------------------------------\n",
    "duplicates = merged_df.duplicated()\n",
    "print(f\"âš  Found {duplicates.sum()} duplicate rows\")\n",
    "\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "print(\"âœ… Duplicates removed. New shape:\", merged_df.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Outlier Treatment & Logging\n",
    "# -------------------------------\n",
    "outlier_cols = [col for col in [\"outlier_zscore\", \"outlier_iforest\", \"outlier_lstm\"] if col in merged_df.columns]\n",
    "\n",
    "if not outlier_cols:\n",
    "    print(\"âš  No outlier flag columns found â€” skipping outlier treatment.\")\n",
    "else:\n",
    "    outlier_summary = {}\n",
    "    for flag_col in outlier_cols:\n",
    "        flagged = merged_df[merged_df[flag_col] == 1]\n",
    "        outlier_summary[flag_col] = len(flagged)\n",
    "        print(f\"âš  Found {len(flagged)} outliers flagged by {flag_col}\")\n",
    "\n",
    "    # Cap numeric outliers at 1stâ€“99th percentiles\n",
    "    for col in num_cols:\n",
    "        lower, upper = np.percentile(merged_df[col], [1, 99])\n",
    "        merged_df[col] = np.clip(merged_df[col], lower, upper)\n",
    "    print(\"âœ… Outliers capped at 1stâ€“99th percentiles!\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Data Quality Report (Mini)\n",
    "# -------------------------------\n",
    "report = {\n",
    "    \"Missing values per column\": merged_df.isnull().sum().to_dict(),\n",
    "    \"Duplicate rows\": duplicates.sum(),\n",
    "    \"Outliers flagged\": outlier_summary\n",
    "}\n",
    "report_path = \"../data/processed/data_quality_report.txt\"\n",
    "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
    "with open(report_path, \"w\") as f:\n",
    "    for k, v in report.items():\n",
    "        f.write(f\"{k}:\\n{v}\\n\\n\")\n",
    "\n",
    "print(f\"ğŸ“ Data quality report saved to {report_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Save cleaned dataset\n",
    "# -------------------------------\n",
    "output_path = \"../data/processed/train_clean.csv\"\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"ğŸ¯ Final cleaned dataset saved to {output_path}\")\n",
    "print(\"Final shape:\", merged_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
